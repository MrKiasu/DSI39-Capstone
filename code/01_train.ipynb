{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSI #39 Capstone Project: Productivity State Developer (PSD) - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code book will do the following:\n",
    "1. Extract the coordinates of the landmarks of the user video.\n",
    "2. Feed the coordinates into multiple classification models for both the productivity and fatigue classes.\n",
    "3. Evaluate the performance of models and output the best performing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca37cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import csv #\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591b9c5",
   "metadata": {},
   "source": [
    "### Defining Drawing and Holistic Solutions from Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce3cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing Helpers - to draw the keypoints and lines on video feed\n",
    "# https://github.com/google/mediapipe/blob/master/mediapipe/python/solutions/drawing_utils.py\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "\n",
    "# Holistic pipeline integrates separate models for pose, face and hand components\n",
    "# Each model is optimised for their particular domain\n",
    "# Read more at https://google.github.io/mediapipe/solutions/holistic.html\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc160d48",
   "metadata": {},
   "source": [
    "### Initiate function to get the headers for the coordinate file (i.e. the x,y,z for all the landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"../data/productive_1.mp4\")\n",
    "\n",
    "# Initiate holistic model - https://google.github.io/mediapipe/solutions/holistic.html\n",
    "# Minimum confidence value ([0.0, 1.0]) from the person-detection model for the detection to be considered successful. Default to 0.5.\n",
    "# Minimum confidence value ([0.0, 1.0]) from the landmark-tracking model for the pose landmarks to be considered tracked successfully. Default to 0.5.\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() # Read Feed\n",
    "\n",
    "        if not ret:\n",
    "            break  # If there are no more frames to read, break out of the loop\n",
    "\n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "        image.flags.writeable = False # Image is no longer writeable     \n",
    "        \n",
    "        # Make Some Detections\n",
    "        results = holistic.process(image) # Make prediction\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True # Image is now writeable\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR CONVERSION RGB 2 BGR\n",
    "        \n",
    "        # 1. Draw face landmarks and face connections\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        # Display the Webcam Capture window with window title and keypoints and connections drawn on the feed\n",
    "        # cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        # Press q key to terminate webcam capture mode\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Once the q key is clicked, close the capture mode and webcam windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate the total number of landmarks detected by Pose model and Face model\n",
    "num_coords = len(results.pose_landmarks.landmark)+len(results.face_landmarks.landmark)\n",
    "\n",
    "# Prepare the list of column names starting with prediction class, coordinates of 1st landmark, coordinates of 2nd landmark, etc\n",
    "landmarks = ['class']\n",
    "\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to get coordinates from a pre-recorded video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4cb2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_coords(cap, class_name,coords_target):\n",
    "\n",
    "    # Initiate holistic model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break  # If there are no more frames to read, break out of the loop\n",
    "            \n",
    "            # Recolor Feed\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False        \n",
    "            \n",
    "            # Make Detections\n",
    "            results = holistic.process(image)\n",
    "            # print(results.face_landmarks)\n",
    "            \n",
    "            # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "            \n",
    "            # Recolor image back to BGR for rendering\n",
    "            image.flags.writeable = True   \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # 1. Draw face landmarks\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                    mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                    mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                    )\n",
    "            \n",
    "            # 2. Right hand\n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                    mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                    mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "            # 3. Left Hand\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                    mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                    mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "            # 4. Pose Detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "            # Export coordinates\n",
    "            try:\n",
    "                # Extract Pose landmarks\n",
    "                pose = results.pose_landmarks.landmark            \n",
    "                pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "                \n",
    "                # Extract Face landmarks\n",
    "                face = results.face_landmarks.landmark\n",
    "                face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "                \n",
    "                # Concate rows\n",
    "                row = pose_row+face_row\n",
    "                \n",
    "                # Append class name \n",
    "                row.insert(0, class_name)\n",
    "                \n",
    "                # Export to CSV\n",
    "                with open(coords_target, mode='a', newline='') as f:\n",
    "                    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                    csv_writer.writerow(row)\n",
    "                \n",
    "            except:\n",
    "\n",
    "                pass\n",
    "                            \n",
    "            cv2.imshow('Raw Webcam Feed', image)\n",
    "            \n",
    "            # Press q key to terminate webcam capture mode\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get coordinates for the productivity (prod) and fatigue (fat) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ddb646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Productive\n",
    "coords_target = \"coords_prod.csv\"\n",
    "\n",
    "if os.path.isfile(\"../data/\" + coords_target):  # delete existing file if found\n",
    "    os.remove(coords_target)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# Initialise CSV\n",
    "with open(coords_target, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/productive_1.mp4\")\n",
    "class_name = \"productive_1\"\n",
    "video_to_coords(cap, class_name, coords_target)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/productive_not_1.mp4\")\n",
    "class_name = \"productive_not\"\n",
    "video_to_coords(cap, class_name, coords_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96528382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Fatigue\n",
    "coords_target = \"coords_fatigue.csv\"\n",
    "\n",
    "if os.path.isfile(\"../data/\" + coords_target):  # delete existing file if found\n",
    "    os.remove(coords_target)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# Initialise CSV\n",
    "with open(coords_target, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/fatigue_1.mp4\")\n",
    "class_name = \"fatigue_1\"\n",
    "video_to_coords(cap, class_name, coords_target)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/fatigue_2.mp4\")\n",
    "class_name = \"fatigue_1\"\n",
    "video_to_coords(cap, class_name, coords_target)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/fatigue_3.mp4\")\n",
    "class_name = \"fatigue_1\"\n",
    "video_to_coords(cap, class_name, coords_target)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/fatigue_not_1.mp4\")\n",
    "class_name = \"fatigue_not_1\"\n",
    "video_to_coords(cap, class_name, coords_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b63c6d",
   "metadata": {},
   "source": [
    "### Model Training - Read coordindates from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b22ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read coordinates csv file\n",
    "df_prod = pd.read_csv('coords_prod.csv')\n",
    "\n",
    "df_prod[\"class\"].replace(\"productive_1\",\"Productive\",inplace=True)\n",
    "df_prod[\"class\"].replace(\"productive_not\",\"Not Productive\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d9d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read coordinates csv file\n",
    "df_fat = pd.read_csv('coords_fatigue.csv')\n",
    "\n",
    "df_fat[\"class\"].replace(\"fatigue_1\",\"Fatigue\",inplace=True)\n",
    "df_fat[\"class\"].replace(\"fatigue_not_1\",\"Not Fatigue\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training - Define pipeline using 4 standard classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbd7775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xy(x_train, y_train):\n",
    "    # Build a pipeline object of different models to test\n",
    "    pipelines = {\n",
    "        'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "        'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "        'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "        'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "    }\n",
    "\n",
    "    # Create a dictionary to store all the pipelines after they have been fitted\n",
    "    fit_models = {}\n",
    "\n",
    "    for algo, pipeline in pipelines.items():\n",
    "        \n",
    "        # [Note] Feature names have also been included in the scaled data so need to use .values. \n",
    "        ## Read more at https://stackoverflow.com/questions/69326639/sklearn-warning-valid-feature-names-in-version-1-0\n",
    "        model = pipeline.fit(x_train.values, y_train.values) \n",
    "        fit_models[algo] = model\n",
    "\n",
    "    return fit_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf20c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_prod.drop('class', axis=1) # Store the Features\n",
    "y = df_prod['class'] # Store the Target value (i.e. Class Name)\n",
    "xprod_train, xprod_test, yprod_train, yprod_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_prod = train_xy(xprod_train, yprod_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2018dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_fat.drop('class', axis=1) # Store the Features\n",
    "y = df_fat['class'] # Store the Target value (i.e. Class Name)\n",
    "xfat_train, xfat_test, yfat_train, yfat_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_fat = train_xy(xfat_train, yfat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f08dc54",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cf2ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.0\n",
      "rc 1.0\n",
      "rf 1.0\n",
      "gb 1.0\n"
     ]
    }
   ],
   "source": [
    "# Display the model test accuracy score for all the fitted models \n",
    "for algo, model in model_prod.items():\n",
    "    yhat = model.predict(xprod_test.values)\n",
    "    print(algo, accuracy_score(yprod_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578fe372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.0\n",
      "rc 1.0\n",
      "rf 1.0\n",
      "gb 1.0\n"
     ]
    }
   ],
   "source": [
    "# Display the model test accuracy score for all the fitted models \n",
    "for algo, model in model_fat.items():\n",
    "    yhat = model.predict(xfat_test.values)\n",
    "    print(algo, accuracy_score(yfat_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf61f9c",
   "metadata": {},
   "source": [
    "### Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e7a5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the chosen pipeline to a .pkl file so that trained model run faster during for real-time predictions\n",
    "with open('productive.pkl', 'wb') as f:\n",
    "    pickle.dump(model_prod['rf'], f)\n",
    "\n",
    "with open('fatigue.pkl', 'wb') as f:\n",
    "    pickle.dump(model_fat['rf'], f)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
