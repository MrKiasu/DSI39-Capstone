{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSI #39 Capstone Project: Productivity State Developer (PSD) - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code book will do the following:\n",
    "1. Extract the coordinates of the landmarks of the user video.\n",
    "2. Feed the coordinates into multiple classification models for both the productivity and fatigue classes.\n",
    "3. Evaluate the performance of models and output the best performing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca37cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import csv #\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591b9c5",
   "metadata": {},
   "source": [
    "### Defining Drawing and Holistic Solutions from Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce3cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing Helpers - to draw the keypoints and lines on video feed\n",
    "# https://github.com/google/mediapipe/blob/master/mediapipe/python/solutions/drawing_utils.py\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "\n",
    "# Holistic pipeline integrates separate models for pose, face and hand components\n",
    "# Each model is optimised for their particular domain\n",
    "# Read more at https://google.github.io/mediapipe/solutions/holistic.html\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc160d48",
   "metadata": {},
   "source": [
    "### Initiate function to get the headers for the coordinate file (i.e. the x,y,z for all the landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"../data/productive_1.mp4\")\n",
    "\n",
    "# Initiate holistic model - https://google.github.io/mediapipe/solutions/holistic.html\n",
    "# Minimum confidence value ([0.0, 1.0]) from the person-detection model for the detection to be considered successful. Default to 0.5.\n",
    "# Minimum confidence value ([0.0, 1.0]) from the landmark-tracking model for the pose landmarks to be considered tracked successfully. Default to 0.5.\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() # Read Feed\n",
    "\n",
    "        if not ret:\n",
    "            break  # If there are no more frames to read, break out of the loop\n",
    "\n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "        image.flags.writeable = False # Image is no longer writeable     \n",
    "        \n",
    "        # Make Some Detections\n",
    "        results = holistic.process(image) # Make prediction\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True # Image is now writeable\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR CONVERSION RGB 2 BGR\n",
    "        \n",
    "        # 1. Draw face landmarks and face connections\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        # Display the Webcam Capture window with window title and keypoints and connections drawn on the feed\n",
    "        # cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        # Press q key to terminate webcam capture mode\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Once the q key is clicked, close the capture mode and webcam windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate the total number of landmarks detected by Pose model and Face model\n",
    "num_coords = len(results.pose_landmarks.landmark)+len(results.face_landmarks.landmark)\n",
    "\n",
    "# Prepare the list of column names starting with prediction class, coordinates of 1st landmark, coordinates of 2nd landmark, etc\n",
    "landmarks = ['class']\n",
    "\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to get coordinates from a pre-recorded video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4cb2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_coords(cap, class_name,coords_target):\n",
    "\n",
    "    # Initiate holistic model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break  # If there are no more frames to read, break out of the loop\n",
    "            \n",
    "            # Recolor Feed\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False        \n",
    "            \n",
    "            # Make Detections\n",
    "            results = holistic.process(image)\n",
    "            # print(results.face_landmarks)\n",
    "            \n",
    "            # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "            \n",
    "            # Recolor image back to BGR for rendering\n",
    "            image.flags.writeable = True   \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # 1. Draw face landmarks\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                    mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                    mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                    )\n",
    "            \n",
    "            # 2. Right hand\n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                    mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                    mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "            # 3. Left Hand\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                    mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                    mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "            # 4. Pose Detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                    )\n",
    "            # Export coordinates\n",
    "            try:\n",
    "                # Extract Pose landmarks\n",
    "                pose = results.pose_landmarks.landmark            \n",
    "                pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "                \n",
    "                # Extract Face landmarks\n",
    "                face = results.face_landmarks.landmark\n",
    "                face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "                \n",
    "                # Concate rows\n",
    "                row = pose_row+face_row\n",
    "                \n",
    "                # Append class name \n",
    "                row.insert(0, class_name)\n",
    "                \n",
    "                # Export to CSV\n",
    "                with open(coords_target, mode='a', newline='') as f:\n",
    "                    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                    csv_writer.writerow(row)\n",
    "                \n",
    "            except:\n",
    "\n",
    "                pass\n",
    "                            \n",
    "            # cv2.imshow('Raw Webcam Feed', image)\n",
    "            \n",
    "            # Press q key to terminate webcam capture mode\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get coordinates for the productivity (prod) and fatigue (fat) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97ddb646",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jacki\\OneDrive\\DSI39-Capstone-Train\\code\\01_train.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m\"\u001b[39m\u001b[39m../data/productive_not_1.mp4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m class_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mproductive_not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m video_to_coords(cap, class_name, coords_target)\n",
      "\u001b[1;32mc:\\Users\\jacki\\OneDrive\\DSI39-Capstone-Train\\code\\01_train.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m        \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Make Detections\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m results \u001b[39m=\u001b[39m holistic\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# print(results.face_landmarks)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Recolor image back to BGR for rendering\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacki/OneDrive/DSI39-Capstone-Train/code/01_train.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m   \n",
      "File \u001b[1;32mc:\\Users\\jacki\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solutions\\holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    137\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n\u001b[0;32m    161\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacki\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    366\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    368\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    369\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    370\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 372\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    373\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    376\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Productive\n",
    "coords_target = \"coords_prod.csv\"\n",
    "\n",
    "if os.path.isfile(\"../data/\" + coords_target):  # delete existing file if found\n",
    "    os.remove(coords_target)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# Initialise CSV\n",
    "with open(coords_target, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/productive_1.mp4\")\n",
    "class_name = \"productive_1\"\n",
    "video_to_coords(cap, class_name, coords_target)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/productive_not_1.mp4\")\n",
    "class_name = \"productive_not\"\n",
    "video_to_coords(cap, class_name, coords_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96528382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Fatigue\n",
    "coords_target = \"coords_fatigue.csv\"\n",
    "\n",
    "if os.path.isfile(\"../data/\" + coords_target):  # delete existing file if found\n",
    "    os.remove(coords_target)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# Initialise CSV\n",
    "with open(coords_target, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/fatigue_1.mp4\")\n",
    "class_name = \"fatigue_1\"\n",
    "video_to_coords(cap, class_name, coords_target)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/fatigue_2.mp4\")\n",
    "class_name = \"fatigue_1\"\n",
    "video_to_coords(cap, class_name, coords_target)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/fatigue_3.mp4\")\n",
    "class_name = \"fatigue_1\"\n",
    "video_to_coords(cap, class_name, coords_target)\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/fatigue_not_1.mp4\")\n",
    "class_name = \"fatigue_not_1\"\n",
    "video_to_coords(cap, class_name, coords_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b63c6d",
   "metadata": {},
   "source": [
    "### Model Training - Read coordindates from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b22ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read coordinates csv file\n",
    "df_prod = pd.read_csv('coords_prod.csv')\n",
    "\n",
    "df_prod[\"class\"].replace(\"productive_1\",\"Productive\",inplace=True)\n",
    "df_prod[\"class\"].replace(\"productive_not\",\"Not Productive\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read coordinates csv file\n",
    "df_fat = pd.read_csv('coords_fatigue.csv')\n",
    "\n",
    "df_fat[\"class\"].replace(\"fatigue_1\",\"Fatigue\",inplace=True)\n",
    "df_fat[\"class\"].replace(\"fatigue_not_1\",\"Not Fatigue\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training - Define pipeline using 4 standard classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xy(x_train, y_train):\n",
    "    # Build a pipeline object of different models to test\n",
    "    pipelines = {\n",
    "        'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "        'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "        'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "        'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "    }\n",
    "\n",
    "    # Create a dictionary to store all the pipelines after they have been fitted\n",
    "    fit_models = {}\n",
    "\n",
    "    for algo, pipeline in pipelines.items():\n",
    "        \n",
    "        # [Note] Feature names have also been included in the scaled data so need to use .values. \n",
    "        ## Read more at https://stackoverflow.com/questions/69326639/sklearn-warning-valid-feature-names-in-version-1-0\n",
    "        model = pipeline.fit(x_train.values, y_train.values) \n",
    "        \n",
    "        fit_models[algo] = model\n",
    "\n",
    "    return fit_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_prod.drop('class', axis=1) # Store the Features\n",
    "y = df_prod['class'] # Store the Target value (i.e. Class Name)\n",
    "xprod_train, xprod_test, yprod_train, yprod_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_prod = train_xy(xprod_train, yprod_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_fat.drop('class', axis=1) # Store the Features\n",
    "y = df_fat['class'] # Store the Target value (i.e. Class Name)\n",
    "xfat_train, xfat_test, yfat_train, yfat_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_fat = train_xy(xfat_train, yfat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f08dc54",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr Train Accuracy 1.0\n",
      "lr Test Accuracy 1.0\n",
      "lr Cross Val Accuracy 1.0\n",
      "lr Time Taken 0.025\n",
      "rc Train Accuracy 1.0\n",
      "rc Test Accuracy 1.0\n",
      "rc Cross Val Accuracy 1.0\n",
      "rc Time Taken 0.018\n",
      "rf Train Accuracy 1.0\n",
      "rf Test Accuracy 1.0\n",
      "rf Cross Val Accuracy 1.0\n",
      "rf Time Taken 0.176\n",
      "gb Train Accuracy 1.0\n",
      "gb Test Accuracy 1.0\n",
      "gb Cross Val Accuracy 1.0\n",
      "gb Time Taken 5.232\n"
     ]
    }
   ],
   "source": [
    "model_time_p =  []\n",
    "\n",
    "for algo, model in model_prod.items():\n",
    "    print(algo, \"Train Accuracy\", accuracy_score(yprod_train, model.predict(xprod_train.values)))   #Train Accuracy\n",
    "    print(algo, \"Test Accuracy\", accuracy_score(yprod_test,model.predict(xprod_test.values)))   #Test Accuracy\n",
    "    print(algo, \"Cross Val Accuracy\", cross_val_score(model , xprod_train ,yprod_train ,cv =5 , scoring='accuracy').mean())   #Test Accuracy\n",
    "\n",
    "    tic = time.time()\n",
    "    model_created = model.fit(xprod_train.values,yprod_train)\n",
    "    toc = time.time()\n",
    "    time_taken = toc - tic\n",
    "    print(algo, \"Time Taken\", \"{:.3f}\".format(time_taken))   #Test Accuracy\n",
    "\n",
    "    model_time_p.append(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr Train Accuracy 1.0\n",
      "lr Test Accuracy 1.0\n",
      "lr Cross Val Accuracy 1.0\n",
      "lr Time Taken 0.027\n",
      "rc Train Accuracy 1.0\n",
      "rc Test Accuracy 1.0\n",
      "rc Cross Val Accuracy 1.0\n",
      "rc Time Taken 0.026\n",
      "rf Train Accuracy 1.0\n",
      "rf Test Accuracy 1.0\n",
      "rf Cross Val Accuracy 1.0\n",
      "rf Time Taken 0.204\n",
      "gb Train Accuracy 1.0\n",
      "gb Test Accuracy 1.0\n",
      "gb Cross Val Accuracy 1.0\n",
      "gb Time Taken 7.426\n"
     ]
    }
   ],
   "source": [
    "model_time_f =  []\n",
    "\n",
    "for algo, model in model_fat.items():\n",
    "    print(algo, \"Train Accuracy\", accuracy_score(yfat_train, model.predict(xfat_train.values)))   #Train Accuracy\n",
    "    print(algo, \"Test Accuracy\", accuracy_score(yfat_test,model.predict(xfat_test.values)))   #Test Accuracy\n",
    "    print(algo, \"Cross Val Accuracy\", cross_val_score(model , xfat_train ,yfat_train ,cv =5 , scoring='accuracy').mean())   #Test Accuracy\n",
    "\n",
    "    tic = time.time()\n",
    "    model_created = model.fit(xfat_train.values,yfat_train)\n",
    "    toc = time.time()\n",
    "    time_taken = toc - tic\n",
    "    print(algo, \"Time Taken\", \"{:.3f}\".format(time_taken))   #Test Accuracy\n",
    "\n",
    "    model_time_f.append(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05126047134399414, 0.04318094253540039, 0.3799927234649658, 12.658194303512573]\n"
     ]
    }
   ],
   "source": [
    "total_time = [ model_time_p[x] + model_time_f[x] for x in range (len (model_time_f))] \n",
    "\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf61f9c",
   "metadata": {},
   "source": [
    "### Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge Classification is chosen as it takes the least amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e7a5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the chosen pipeline to a .pkl file for deployment\n",
    "with open('productive.pkl', 'wb') as f:\n",
    "    pickle.dump(model_prod['rc'], f)\n",
    "\n",
    "with open('fatigue.pkl', 'wb') as f:\n",
    "    pickle.dump(model_fat['rc'], f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
